# GradientDescent
Performing Gradient descent for real valued functions. (https://en.wikipedia.org/wiki/Gradient_descent)
The bulk of this file is the function `gradient.descent`, which performs gradient based algorithm in order to obtain local extrema of a function. We now discus each of its parameters:
  - `f` : the function in which we want to perform gradient descent (if the function is higher dimensional the function should still only have one parameter, and each variable be a component of said parameter, this is instead of `function(x,y){x+y}` one must use `function(x){x[1]+x[2]}`
  -  `gradf` : the gradient of `f` as a function, i the same form as `f`
  -  `x0` : starting point for initializing our algorithm
  -  `iterations` : total number of iterations we want to perform
  -  `eta` : the step size for our algorithm
  -  `momentum` : by default is 0, and so it performs regular gradient descent, if `momentum`is set to a number bigger than zero it performs gradient descent with momentum (with the parameter of momentum being this number) (https://www.scaler.com/topics/momentum-based-gradient-descent/)
  -  `ascent` : boolean value set to `FALSE` by default, if set to true the algorithm will perform gradient ascent to try to obtain a local maximum.
  -  `xlim` : if plots can be generated they are generated by always having the point given by the algorithm in the center of the plot, the range of x in the plot will be x* - `xlim` to x* + `xlim` where x* is the extremum given by gradient descent
  -  `ylim` : idem for the y coordinates, by default it is the same as `xlim`
  -  `detail` : when plotting the function the vector used as support (or grid in the two dimensional case) is a vector generated by `seq`, `detail` is its `by=` parameter

Some functions to run as examples are added, among them the classical Rosenbrock function typical for testing Gradient based algorithms, as well as its high dimensional generalisation. (https://arxiv.org/pdf/2101.10546.pdf)
